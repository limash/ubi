{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get hardware strategy\n",
    "def get_hardware_strategy():\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_bfloat16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "    return tpu, strategy\n",
    "\n",
    "tpu, strategy = get_hardware_strategy()\n",
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2048 * strategy.num_replicas_in_sync\n",
    "# Learning rate\n",
    "LR = 0.001\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "FEATURES = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    \"target\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    \"features\": tf.io.FixedLenFeature([FEATURES], tf.float32),\n",
    "}\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    example = tf.io.parse_single_example(example, feature_dict)\n",
    "    X = example[\"features\"]\n",
    "    y = example[\"target\"]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = tf.io.gfile.glob(\"data/tfrecords/train/fold0/\" + \"*.tfrec\")\n",
    "train_filenames_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\n",
    "train_filenames_ds = train_filenames_ds.shuffle(len(train_filenames), reshuffle_each_iteration=True)\n",
    "train_dataset = train_filenames_ds.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
    "                                              cycle_length=5,\n",
    "                                              num_parallel_calls=AUTO)\n",
    "train_dataset = train_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "train_dataset = train_dataset.shuffle(100000, reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filenames = tf.io.gfile.glob(\"data/tfrecords/validation/fold0/\" + \"*.tfrec\")\n",
    "valid_filenames_ds = tf.data.Dataset.from_tensor_slices(valid_filenames)\n",
    "valid_filenames_ds = valid_filenames_ds.shuffle(len(valid_filenames), reshuffle_each_iteration=True)\n",
    "valid_dataset = valid_filenames_ds.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
    "                                              cycle_length=5,\n",
    "                                              num_parallel_calls=AUTO)\n",
    "valid_dataset = valid_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse an example\n",
    "# ds = tf.data.TFRecordDataset('data/tfrecords/train/fold0/0.tfrec')\n",
    "# iterator = iter(ds)\n",
    "# raw_example = next(iterator)\n",
    "# example = tf.io.parse_single_example(raw_example, feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape):\n",
    "    with strategy.scope(): \n",
    "        def fc_block(x, units):\n",
    "            x = tf.keras.layers.Dropout(0.35)(x)\n",
    "            x = tf.keras.layers.Dense(units, activation = 'relu')(x)\n",
    "            return x\n",
    "        \n",
    "        inp = tf.keras.layers.Input((shape))\n",
    "        x = fc_block(inp, units = 768)\n",
    "        x = fc_block(x, units = 384)\n",
    "        x = fc_block(x, units = 192)\n",
    "        output = tf.keras.layers.Dense(1, activation = 'linear')(x)\n",
    "        model = tf.keras.models.Model(inputs = [inp], outputs = [output])\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "        model.compile(\n",
    "            optimizer = opt,\n",
    "            loss = [tf.keras.losses.MeanSquaredError()],\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "model = build_model(FEATURES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"data/keras_models/model_{index}\", save_best_only=True)\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=VERBOSE,\n",
    "                    validation_data=valid_dataset, \n",
    "                    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "valid_df = pd.read_pickle(\"data/tfrecords/validation/fold0/validation.pkl\")\n",
    "features = [col for col in valid_df.columns if col not in ['row_id', 'time_id', 'investment_id', 'target']]\n",
    "x_val = valid_df[features]\n",
    "model = tf.keras.models.load_model(f\"data/keras_models/model_{index}\")\n",
    "val_pred = model.predict(x_val, batch_size = BATCH_SIZE).astype(np.float32).reshape(-1)\n",
    "valid_df['prediction'] = val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pearson correlation coefficient\n",
    "def pearson_coef(data):\n",
    "    return data.corr()['target']['prediction']\n",
    "\n",
    "# Calculate mean pearson correlation coefficient\n",
    "def comp_metric(valid_df):\n",
    "    return np.mean(valid_df.groupby(['time_id']).apply(pearson_coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_metric(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6363654707a3d37b300368bbc9381cc10fc6ca39658e75fc3e05c7cbeb1913f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
